\cleardoublepage

\chapter{Trabajo del Alumno}
\label{makereference11}

\section{Pablo Aragón Moreno}
\subsection{Investigación}
Tras la primera reunión con los directores acerca del proyecto, el alumno Pablo Aragón Moreno se asignó la investigación de la recogida y tratamiento de los datos, tanto de InfoRiego, como la de los datos meteorológicos del exterior.

Investigó distintas tecnologías para dicho cometido: Ruby, Python, C... Para ello estudió estas posibilidades mediante su documentación, experiencia y opiniones en Internet.

Ya que el algoritmo de predicción y el entrenamiento se iba a realizar en Python, Pablo Aragón Moreno decidió ser consistente con el proyecto y eligió dicho lenguaje. Además, Python es un lenguaje muy potente en lo que al WebScrapping se refiere, dado que tiene muchas librerías de tratamiento y obtención de grandes cantidades de datos.

Pablo Aragón Moreno siguió con su investigación para la recogida de datos meteorológicos. La idea era desarrollar un módulo en la Raspberry que controle y orqueste la obtención de datos mediante dos sensores:

\begin{itemize}
\item DHT22: Sensor de temperatura y humedad.
\item SP-212: Piranómetro.
\item ADC MCP3008: Conversor analógico digital.
\end{itemize}

El alumno estudió los ``datasheets'' y documentación de los sensores para decantarse por una tecnología.

Debido a que los fabricantes de los sensores proporcionan librerías en Python para el fácil y cómodo manejo de los dispositivos, y debido a la predisposición de Pablo a elegir dicho lenguaje, la elección de la tecnología en este caso fue muy sencilla.

\subsection{Desarrollo}
Pablo Aragón Moreno comenzó a desarrollar los scripts en Python de recogida de datos de la web InfoRiego.

El trabajo consistía en la recopilación de los datos del servidor de InfoRiego, después descomprimirlos al formato original (CSV) y el tratamiento y clasificación en otros CSVs para así poder normalizar y generar los conjuntos de datos para el ``Machine Learning''. Esta labor facilitó mucho la tarea de Abel Coronado López para entrenar dichos datos y, más adelante, predecir.

Este desarrollo del tratamiento de datos permitió que se pudiesen formar conjuntos de datos para el entrenamiento de forma parametrizada. Permite indicar qué intervalo de tiempo se requiere y el algoritmo se encarga de comprobar si ya está descargado y en caso contrario lo descarga y posteriormente generar el resto de conjuntos de datos a partir de este que se requieran para el entrenamiento también de forma parametrizada.

Conseguido ese hito, el alumno implementó los algoritmos de recogida de datos meteorológicos del nodo. Para ello se valió de las librerías mencionadas anteriormente.

\subsection{Documentación}
Durante su desarrollo, Pablo Aragón Moreno fue documentando todo su trabajo, tanto en Drive, como en Github.
\begin{itemize}
\item En Drive documentó todo aquello relacionado con la tecnología usada (links, trabajos...).
\item En Github explicó cómo se instala el nodo: cableado, scripts de configuración... Entre otras cosas. 
\end{itemize}
La última fase del proyecto consistió en documentar toda su parte, tanto de investigación como de desarrollo, en la memoria final. A parte, ayudó a sus compañeros a desarrollar otras secciones... 

\section{María Castañeda López}
\subsection{Investigación}
Al comenzar el proyecto, María Castañeda López se estuvo informando de algunos protocolos para poder recoger, almacenar y visualizar los datos que provinieran del nodo (CoAP, MQTT o AMQP). En especial investigó MQTT, muy utilizado en la comunicación máquina a máquina, muy simple. Aunque María Castañeda López y sus compañeros se informaron sobre otras alternativas, MQTT y el servidor Mosquitto fue el elegido, principalmente porque este fue el recomendado por sus tutores. 

Otra parte importante que María Castañeda López tuvo que investigar al principio del proyecto, fue la manera en la que el nodo se comunicaba con MQTT. Como Raspberry 3 incluye un módulo de Wi-Fi, esta fue la tecnología que decidió utilizar. Se estudiaron otras alternativas como Lora, la cual utiliza menos recursos y es la más utilizada en sistemas ``machine-to-machine'', como nuestro proyecto. Al ser sólo un prototipo, decidieron no preocuparse por la optimización de recursos y optar por aprovechar el modulo Wi-Fi nativo en la Raspberry.

La alumna implementó la otra parte de la conexión MQTT, el servidor de predicción. Utiliando la funcionalidad del cliente MQTT implementado en el nodo, lo acopló al sistema del servidor.

Respecto al sistema de visualización, María Castañeda López estudió los sistemas disponibles en este momento en el mercado, como por ejemplo: FreeBoard o IoTDataViz o, incluso, uno desarrollado por ellos mismos. Pero la alumna y sus compañeros decidieron una vez más, hacer caso a las recomendaciones de sus tutores y utilizar ThinkSpeak. Esta herramienta, además de ser gratuita, te permite la facilidad de manipular los datos de una manera más personalizada. Para comunicarse con ThingSpeak, se decidió desarrollar un módulo denominado ``Bridge'' para poder mandar los resultados de nuestra predicción al sistema de visualización. Este Bridge aprovecha la API HTTP que proporciona ThingSpeak para realizar la comunicación

\subsection{Desarrollo}
Una vez que María Castañeda López terminó el trabajo de investigación, ella y sus compañeros desarrollaron una serie de scripts relacionados con la comunicación entre los componentes del sistema.

Uno de estos scripts se encarga de la comunicación MQTT del nodo con el servidor de datos mediante la ayuda de la librería paho-mqtt, que facilitó mucho las cosas.

Además María Castañeda López y sus compañeros decidieron crear tres logs, dos en el nodo y otro en el servidor de predicción, para, tener así, un método de registro de los eventos futuros.

En los logs del nodo se almacenan datos recogidos por el nodo en forma de registros diarios para prevenir su pérdida. Como el cliente MQTT está escuchando continuamente, en cuanto recibe los datos, los preservará en dicho log. También almacena el resultado de cada conexión con el broker permitiendo así conocer si la conexión falló en algún momento.

Por otro lado, el log del servidor de predicción guarda todos los datos que recibe para poder utilizarlos en la predicción.

\subsection{Documentación}

La alumna María Castañeda López, fue recopilando documentación sobre los protocolos y tecnologías investigadas para poder desarrollar su trabajo durante todo el proyecto. Y, al igual que el resto de sus compañeros, almacenó dicha información en una carpeta compartida por el equipo de desarrollo del proyecto y en Github. De está manera podían tener tanto ella, como el resto de sus compañeros, la documentación, problemas, comentarios o dudas de una manera fácil y directa.

María Castañeda López se ha encargado de redactar en este documento todo lo relacionado con su parte de la investigación y del desarrollo, además de aportar su ayuda junto a la del resto de sus compañeros en otros capítulos de este documento.

\section{Abel Coronado López}
\subsection{Investigación}

Durante la fase inicial del proyecto, Abel Coronado López se encargó de la investigación de las distintas tecnologías para desarrollar ``Machine Learning'' que se encontraban actualmente en el ``mercado'' y seleccionar cuál era la que mejor le venía al proyecto, y cuáles eran los motivos por los que había escogido la tecnología.

En un principio, la tecnología a utilizar sería MATLAB, pero, después de la investigación previa, se descubrió que Python también tiene una gran potencia en este sector. Python cuenta con un gran abanico de librerías para trabajar con grandes cantidades de datos, análisis científico, inteligencia artificial... Cuenta también con distribuciones que ya incluyen gran cantidad de estas librerías para poder hacer uso de ellas sin necesidad de instalarlas una a una como ``Anaconda''. Además, junto con la facilidad de programar en este lenguaje y la experiencia previa de los componentes del grupo con él, hizo que esta fuera la elección final.

Tras tener elegida la tecnología con la que desarrollar el proyecto, Abel Coronado López, investigó distintos algoritmos matemáticos para el ``Machine Learning'' y cómo poder implementarlos.

Hizo uso de cursos online, como por ejemplo \href{https://www.udacity.com}{Udacity}, y se valió de tutoriales básicos de ``Machine Learning'' en \href{https://www.youtube.com}{YouTube} para coger una idea básica de este nuevo paradigma emergente.

Con una idea más clara de los objetivos del proyecto y de cómo implementarlos, el alumno estudió pequeños problemas resueltos relacionados con la IA propuestos en Intenet:

\begin{itemize}
\item Desastre del Titanic
\item Investigación de la diabetes
\item Predicción del divorcio
\end{itemize}

Cuando la fase de estudio de los algoritmos concluyó, el alumno, junto al resto de componentes del equipo, generó una lista de posibles algoritmos para utilizar en el proyecto: 

\begin{itemize}
\item Regresión Lineal
\item SVM
\item Clasificador
\end{itemize}

Esta lista iría cambiando según avanzara el proyecto debido a diversos factores:

\begin{itemize}
\item Poca documentación
\item Problemas de implementación
\item Malos resultados
\end{itemize}

En esta fase de investigación, se descubrió un método de estudio para automatizar las pruebas y hacer más viable la experimentación de modelos y parámetros usados en el ``Machine Learning'': Grid Search \ref{makereference5.3}.

\subsection{Desarrollo}
Con la fase de investigación finalizada, Abel Coronado López, junto con el resto de compañeros, empezaron con el desarrollo e implementación de estos algoritmos en el proyecto.

Gracias a Grid Seach este trabajo fue mucho más viable y cómodo.

Se desarrolló una plantilla que usa Grid Search para abstraer los modelos y parámetros a explorar. De esta manera, la fase de entrenamiento sería configurable mediante archivos ``JSON''. El alumno desarrolló dichos ficheros explicados previamente. Ver figura \ref{json}.

Una vez finalizada la fase de entrenamiento con distintos modelos y configuraciones, el equipo eligió el algoritmo que mejor precisión de predicción tiene para nuestro contexto.

\subsection{Documentación}
Durante todos los procesos anteriormente descritos, el alumno Abel Coronado López documentó todo lo relacionado con las tecnologías investigadas y utilizadas por el sistema en una carpeta compartida por el equipo de desarrollo del proyecto, y en Github. Esta documentación consistía en comentar todo lo que se iba desarrollando e investigando, de forma que los demás compañeros, si querían introducirse en esa parte del sistema, únicamente tendrían que leer dónde y cómo se hizo.

En la fase final del proyecto, el alumno ha documentado todo el sistema relacionado con la predicción para que este fuese introducido en la memoria del proyecto, también a aportado en otras secciones como la introducción, el nodo, etc...